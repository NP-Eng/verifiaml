{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import log2, ceil, floor\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(HOME_DIR, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_nearest(x, scale, zero, qtype):\n",
    "    if qtype not in {np.int8, np.uint8}:\n",
    "        raise Exception(\"Only quantization to int8 or uint8 is supported\")\n",
    "    \n",
    "    (min, max) = (-128, 127) if qtype == np.int8 else (0, 255)\n",
    "\n",
    "    return np.clip(np.rint(x / scale) + zero, min, max).astype(qtype)\n",
    "\n",
    "def fc_and_requantize(input_tensor, weights, bias, q_i, q_w, q_o):\n",
    "    \n",
    "    if input_tensor.dtype != np.int8:\n",
    "        raise Exception(\"Input must be of type int8\")\n",
    "    \n",
    "    if weights.dtype != np.int8:\n",
    "        raise Exception(\"Weights must be of type int8\")\n",
    "    \n",
    "    if bias.dtype != np.int32:\n",
    "        raise Exception(\"Input and weights must be of type int32\")\n",
    "    \n",
    "    (s_i, z_i), (s_w, z_w), (s_o, z_o) = q_i, q_w, q_o\n",
    "    \n",
    "    if z_w != 0:\n",
    "        raise Exception(\"Expected zero point of weights to be 0\")\n",
    "\n",
    "    s = s_i * s_w / s_o\n",
    "\n",
    "    # 1) shift input tensor\n",
    "    input_tensor_32 = input_tensor.astype(np.int32) - z_i\n",
    "    weights_32 = weights.astype(np.int32)\n",
    "\n",
    "    # 2) compute the bmm\n",
    "    bmm = np.matmul(input_tensor_32, weights_32.transpose()) + bias\n",
    "\n",
    "    # 3) requantize\n",
    "    raise Exception(\"Change to accurate rounding\")\n",
    "    rq = np.rint(s * bmm) + z_o\n",
    "\n",
    "    # 4) saturating cast\n",
    "    output = np.clip(rq, -128, 127).astype(np.int8)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train.astype(np.float32) / 255.0, x_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full model\n",
    "\n",
    "(not ready; go to \"Simple model\" below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(os.path.join(MODEL_DIR, \"mnist_model_quant.tflite\"))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tensors = [None] * 11\n",
    "\n",
    "for i in range(11):\n",
    "    try:\n",
    "        initial_tensors[i] = interpreter.get_tensor(i).copy()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "print(input_details)\n",
    "print(input_details[\"dtype\"])\n",
    "\n",
    "print(output_details)\n",
    "print(output_details[\"dtype\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_image = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = x_test[chosen_image]\n",
    "\n",
    "# Need to quantize the inputs outside the model!\n",
    "input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "\n",
    "# quantisation transformation as float32 first\n",
    "test_image = quantize_nearest(test_image, input_scale, input_zero_point, np.uint8)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "output_prediction = output.argmax()\n",
    "\n",
    "print(output)\n",
    "print(\"{} (correct: {})\".format(y_test[chosen_image], output_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: to perform a saturating cast, one must use np.clip. Otherwise problematic things happen - for instance, from f32 to u8, it seems first the floor is applied followed by % 256 (which is not what we want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_200 = np.array([200], dtype=np.float32)\n",
    "print(arr_200.astype(np.uint8))\n",
    "\n",
    "arr_m42 = np.array([-42], dtype=np.float32)\n",
    "print(arr_m42.astype(np.uint8))\n",
    "print(np.clip(arr_m42, 0, 255).astype(np.uint8))\n",
    "\n",
    "arr_422 = np.array([422], dtype=np.float32)\n",
    "print(arr_422.astype(np.uint8))\n",
    "print(np.clip(arr_422, 0, 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The tensors in the interpreter (cf. next cell) should be interpreted as follows:\n",
    "- 0: `serving_default_flatten_3_input:0`: it simply holds the (already quantised) input tensor (u8, initialised to 0)\n",
    "- 1: `sequential_6/flatten_3/Const`: it stores, as a constant, the shape that the input should be flattened to by the Reshape node (cf. 7), to which it is an input (i32, does not change)\n",
    "- 2: `sequential_6/dense_7/BiasAdd/ReadVariableOp`: it holds the bias for the second FC layer (identified by `dense_7`), and it consists of 10 `i32`s\n",
    "- 3: `sequential_6/dense_7/MatMul`: this is the vec-by-matrix multiplication for the second FC layer. It holds the matrix coefficients as with entries in `i8`. The vector's has entries in ???. The two are multiplied together in `i32` precision to avoid overflows.\n",
    "- 4: `sequential_6/dense_6/BiasAdd/ReadVariableOp`: it holds the bias for the first FC layer  (cf. 2)\n",
    "- 5: `sequential_6/dense_6/MatMul`: this is the vec-by-matrix multiplication for the first FC layer (cf. 3)\n",
    "- 6: `tfl.quantize`: this has the exact same quantisation scale as the input node, but the zero point is -128 as opposed to 0. Also, I am unsure what it does, since input quantisation needs to be performed externally by the user... (it changes during inference!)\n",
    "- 7: `sequential_6/flatten_3/Reshape`: it flattens the 28 x 28 image into a flat 784-element vector (no value)\n",
    "- 8: `sequential_6/dense_6/MatMul;sequential_6/activation_3/Relu;sequential_6/dense_6/BiasAdd`: this performs BMM and ReLU (no value)\n",
    "- 9: `StatefulPartitionedCall:01`: ??? (i8, initial value: 0)\n",
    "- 10: `StatefulPartitionedCall:0`: holds the actual output (u8, initial value: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tensors = [None] * 11\n",
    "\n",
    "for i in range(11):\n",
    "    try:\n",
    "        final_tensors[i] = interpreter.get_tensor(i).copy()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"bias2\": interpreter.get_tensor(2).copy(),\n",
    "    \"mat2\": interpreter.get_tensor(3).copy(),\n",
    "    \"bias1\": interpreter.get_tensor(4).copy(),\n",
    "    \"mat1\": interpreter.get_tensor(5).copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_i1 = interpreter.get_tensor_details()[0][\"quantization\"][0]\n",
    "s_w1 = interpreter.get_tensor_details()[5][\"quantization\"][0]\n",
    "s_o1 = interpreter.get_tensor_details()[8][\"quantization\"][0]\n",
    "s_i1, s_w1, s_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_i2 = interpreter.get_tensor_details()[8][\"quantization\"][0]\n",
    "s_w2 = interpreter.get_tensor_details()[3][\"quantization\"][0]\n",
    "s_o2 = interpreter.get_tensor_details()[9][\"quantization\"][0]\n",
    "s_i2, s_w2, s_o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = test_image\n",
    "flattened_input = input_tensor.flatten()\n",
    "# TODO I think the next two lines do the same as the third on its own\n",
    "precision_input = flattened_input.astype(np.int32)\n",
    "quantised_input = precision_input - 128\n",
    "finalised_input = quantised_input.astype(np.int8)\n",
    "\n",
    "fc1 = fc_and_requantize(finalised_input, params[\"mat1\"], params[\"bias1\"], s_i1, s_w1, s_o1)\n",
    "\n",
    "# Applying ReLU to i8 input\n",
    "relu = fc1.clip(0, 127)\n",
    "\n",
    "fc2 = fc_and_requantize(relu, params[\"mat2\"], params[\"bias2\"], s_i2, s_w2, s_o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = (np.matmul(finalised_input.astype(np.int32), params[\"mat1\"].astype(np.int32).transpose()) + params[\"bias1\"]) * s_i1 * s_w1 / s_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = v.clip(-128, 127).astype(np.int8).clip(0, 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.matmul(v2.astype(np.int32), params[\"mat2\"].astype(np.int32).transpose()) + params[\"bias2\"]) * s_i2 * s_w2 / s_o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.get_tensor(9)\n",
    "fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, fz = interpreter.get_tensor_details()[9][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, fz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2/fs + fz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.get_tensor(9) - interpreter.get_tensor(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(os.path.join(MODEL_DIR, \"simple_model_quant.tflite\"), experimental_preserve_all_tensors=True)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "print(input_details)\n",
    "print(input_details[\"dtype\"])\n",
    "\n",
    "print(output_details)\n",
    "print(output_details[\"dtype\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_image = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = x_test[chosen_image]\n",
    "\n",
    "# Need to quantize the inputs outside the model!\n",
    "input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "input_tensor = quantize_nearest(test_image, input_scale, input_zero_point, np.uint8)\n",
    "input_tensor = np.expand_dims(input_tensor, axis=0)\n",
    "\n",
    "# Run the model\n",
    "interpreter.set_tensor(input_details[\"index\"], input_tensor)\n",
    "interpreter.invoke()\n",
    "tflite_output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "output_prediction = tflite_output.argmax()\n",
    "\n",
    "print(tflite_output)\n",
    "print(\"{} (correct: {})\".format(output_prediction, y_test[chosen_image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.get_tensor_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(interpreter.get_tensor_details()):\n",
    "    print(i, \": \", t[\"name\"], sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter._get_ops_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_idx = 5\n",
    "bias_idx = 2\n",
    "weight_idx = 3\n",
    "output_idx = 6\n",
    "\n",
    "w = interpreter.get_tensor(weight_idx)\n",
    "b = interpreter.get_tensor(bias_idx)\n",
    "q_i = interpreter.get_tensor_details()[input_idx][\"quantization\"]\n",
    "q_w = interpreter.get_tensor_details()[weight_idx][\"quantization\"]\n",
    "q_o = interpreter.get_tensor_details()[output_idx][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) flatten input\n",
    "flattened_input = input_tensor.reshape(interpreter.get_tensor(1)) # [-1, 784]\n",
    "\n",
    "# 2) shift input tensor by -128 to sitch from input type (uint8) to TF Lite internal type (int8) \n",
    "finalised_input = flattened_input.astype(np.int32)\n",
    "finalised_input = finalised_input - 128\n",
    "finalised_input = finalised_input.astype(np.int8)\n",
    "\n",
    "# 3) run fully-connected layer\n",
    "fc1 = fc_and_requantize(finalised_input, w, b, q_i, q_w, q_o)\n",
    "\n",
    "# 4) undo the shift to switch from TF Lite internal type (int8) to output type (uint8)\n",
    "manual_output = fc1.astype(np.int32)\n",
    "manual_output = manual_output + 128\n",
    "manual_output = manual_output.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(manual_output == tflite_output).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more optimised version of the simple model (TF Lite and manual) to meaningfully compare execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I_S, I_Z = input_details[\"quantization\"]\n",
    "# RESHAPE = interpreter.get_tensor(1)\n",
    "\n",
    "# W_32 = interpreter.get_tensor(3).transpose().astype(np.int32)\n",
    "# B_32 = interpreter.get_tensor(2).astype(np.int32)\n",
    "# (S_I, Z_I) = interpreter.get_tensor_details()[5][\"quantization\"]\n",
    "# (S_W, Z_W) = interpreter.get_tensor_details()[3][\"quantization\"]\n",
    "# (S_O, Z_O) = interpreter.get_tensor_details()[6][\"quantization\"]\n",
    "# S = S_I * S_W / S_O\n",
    "\n",
    "# def quantise_input(x):\n",
    "#     x_q = quantize_nearest(x, I_S, I_Z, np.uint8)\n",
    "#     return np.expand_dims(x_q, axis=0)\n",
    "\n",
    "# def manual_model(x):\n",
    "#     x = (x.reshape(RESHAPE).astype(np.int32) - 128).astype(np.int8)\n",
    "#     x = x.astype(np.int32) - Z_I\n",
    "#     x = np.matmul(x, W_32) + B_32\n",
    "#     x = np.clip(np.rint(S * x) + Z_O, -128, 127)\n",
    "#     x = (x + 128).astype(np.uint8)\n",
    "\n",
    "#     return x\n",
    "\n",
    "# quantised_test_x = [quantise_input(x) for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_S, I_Z = input_details[\"quantization\"]\n",
    "RESHAPE = interpreter.get_tensor(1)\n",
    "\n",
    "W_32 = interpreter.get_tensor(3).transpose().astype(np.int32)\n",
    "B_32 = interpreter.get_tensor(2).astype(np.int32)\n",
    "(S_I, Z_I) = interpreter.get_tensor_details()[5][\"quantization\"]\n",
    "(S_W, Z_W) = interpreter.get_tensor_details()[3][\"quantization\"]\n",
    "(S_O, Z_O) = interpreter.get_tensor_details()[6][\"quantization\"]\n",
    "\n",
    "def quantise_input(x):\n",
    "    x_q = quantize_nearest(x, I_S, I_Z, np.uint8)\n",
    "    return np.expand_dims(x_q, axis=0)\n",
    "\n",
    "quantised_test_x = [quantise_input(x) for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-scaling computation\n",
    "\n",
    "# Fun fact: changing to the following makes S exactly equal to S_UINT / (2**S_SHIFT)\n",
    "# ROUNDING_PRECISION = 64\n",
    "# APP_S_TYPE = np.uint64\n",
    "\n",
    "ROUNDING_PRECISION = 32\n",
    "APP_S_TYPE = np.int32 # as in gemmlowp's SaturatingRoundingDoublingHighMul\n",
    "\n",
    "def approximate_rescaling_factor(s):\n",
    "    # negative scale, aside from theoretically never happening, would break our rounding assumption below\n",
    "    if s < 0:\n",
    "        raise Exception(\"s must be positive\")\n",
    "    if s > 1:\n",
    "        raise Exception(\"Make sure s > 1 is handled correctly\")\n",
    "    \n",
    "    c = ceil(log2(s) + 1)\n",
    "\n",
    "    scaled_s = floor(2**(ROUNDING_PRECISION - c) * s)\n",
    "    rounding_bit = scaled_s & 1\n",
    "\n",
    "    int_s = (scaled_s >> 1) + rounding_bit\n",
    "\n",
    "    return (ROUNDING_PRECISION - 1 - c, APP_S_TYPE(int_s))\n",
    "\n",
    "def round_float_half_away_from_zero(f):\n",
    "    f_abs = np.abs(f)\n",
    "    f_abs_floor = np.floor(f_abs)\n",
    "    rounding_bit = 1 if (f_abs - f_abs_floor) >= 0.5 else 0\n",
    "\n",
    "    return np.sign(f) * (f_abs_floor + rounding_bit)\n",
    "\n",
    "def new_approximate_rescaling_factor(s1, s2, s3):\n",
    "\n",
    "    # TODO we are omitting some of the checks\n",
    "\n",
    "    if s1 == 0 or s2 == 0:\n",
    "        print(\"Warning: Rescaling multiplier equal to 0 found\")\n",
    "        return 0, 0\n",
    "\n",
    "    s1, s2, s3 = np.float64(s1), np.float64(s2), np.float64(s3)\n",
    "\n",
    "    s = s1 * s2 / s3\n",
    "\n",
    "    # negative scale, aside from theoretically never happening, would break our rounding assumption below\n",
    "    if s < 0:\n",
    "        raise Exception(\"s must be positive\")\n",
    "    if s > 1:\n",
    "        raise Exception(\"Make sure s > 1 is handled correctly\")\n",
    "    \n",
    "    # assuming TFLITE_EMULATE_FLOAT = false, since our system can actually run floating-point arithmetic\n",
    "    exp = floor(log2(s)) + 1\n",
    "    signif = s * (1 << -exp)\n",
    "\n",
    "    q_signif = round_float_half_away_from_zero(signif * (1 << 31)).astype(np.int64)\n",
    "\n",
    "    # TODO can this happen?\n",
    "    if (q_signif == (1 << 31)):\n",
    "        q_signif /= 2\n",
    "        exp += 1\n",
    "\n",
    "    if exp < -31:\n",
    "        exp = 0\n",
    "        q_signif = 0\n",
    "    \n",
    "    # I have no idea if our build has single rounding\n",
    "    # #if TFLITE_SINGLE_ROUNDING\n",
    "    #    // Single-rounding MultiplyByQuantizedMultiplier doesn't support a shift > 30,\n",
    "    #    // saturate it.\n",
    "    #    if (*shift > 30) {\n",
    "    #    *shift = 30;\n",
    "    #    q_fixed = (1LL << 31) - 1;\n",
    "    #    }\n",
    "    # #endif\n",
    "\n",
    "    q_signif = q_signif.astype(np.int32)\n",
    "\n",
    "    return exp, q_signif\n",
    "\n",
    "S_REL_SHIFT, S_UINT = new_approximate_rescaling_factor(S_I, S_W, S_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_nearest_half_up(n, shift):\n",
    "    return (n + (1 << (shift - 1))) >> shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO one could wrap this in type checks for good measure (one per tensor, not per element)\n",
    "ROUNDING = round_nearest_half_up\n",
    "\n",
    "# def requantise_half_away_from_zero(x):\n",
    "#     # TODO control overflows here?\n",
    "#     abs_a_s_int = np.abs(x) * S_UINT\n",
    "#     rounding_bit = (abs_a_s_int >> (S_SHIFT - 1)) & 1\n",
    "#     sh = (abs_a_s_int >> S_SHIFT)\n",
    "\n",
    "#     return np.sign(x) * (sh + rounding_bit)\n",
    "\n",
    "# TODO there's probably a more elegant way to do this\n",
    "def requantise(x):\n",
    "    # TODO control overflows here or in the ROUNDING function?\n",
    "    return ROUNDING(x * S_UINT, S_SHIFT)\n",
    "\n",
    "# requantise_tensor = np.vectorize(requantise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline int32 MultiplyByQuantizedMultiplier( int32 x, \n",
    "#                                             int32 quantized_multiplier,\n",
    "#                                             int shift) {\n",
    "#   using gemmlowp::RoundingDivideByPOT;\n",
    "#   using gemmlowp::SaturatingRoundingDoublingHighMul;\n",
    "\n",
    "\n",
    "#   int left_shift = shift > 0 ? shift : 0;\n",
    "#   int right_shift = shift > 0 ? 0 : -shift;\n",
    "  \n",
    "#   return RoundingDivideByPOT(\n",
    "    \n",
    "#             SaturatingRoundingDoublingHighMul(\n",
    "#                                  x * (1 << left_shift), quantized_multiplier\n",
    "#             ),\n",
    "\n",
    "#         right_shift);\n",
    "# }\n",
    "\n",
    "# The shift arg above is to be understod as: to the left (by the two ternary assignments)\n",
    "\n",
    "# In our case, the shift is always to the right, so:\n",
    "#     - right_shift is set to the additive inverese of our shift\n",
    "#     - left_shift is set to 0\n",
    "\n",
    "#   return RoundingDivideByPOT(\n",
    "    \n",
    "#         SaturatingRoundingDoublingHighMul(\n",
    "#                                 x * 1, quantized_multiplier\n",
    "#         ),\n",
    "\n",
    "#     right_shift);\n",
    "\n",
    "\n",
    "# https://github.com/google/gemmlowp/blob/master/fixedpoint/fixedpoint.h#L302\n",
    "# https://github.com/google/gemmlowp/blob/master/fixedpoint/fixedpoint.h#L340\n",
    "# // This function implements the same computation as the ARMv7 NEON VQRDMULH\n",
    "# // instruction.\n",
    "# inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,\n",
    "#                                                       std::int32_t b) {\n",
    "#   bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();\n",
    "#   std::int64_t a_64(a);\n",
    "#   std::int64_t b_64(b);\n",
    "#   std::int64_t ab_64 = a_64 * b_64;\n",
    "#   std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));\n",
    "#   std::int32_t ab_x2_high32 =\n",
    "#       static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));\n",
    "#   return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;\n",
    "# }\n",
    "\n",
    "# inline IntegerType RoundingDivideByPOT(IntegerType x, ExponentType exponent) {\n",
    "#   assert(exponent >= 0);\n",
    "#   assert(exponent <= 31);\n",
    "#   const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);\n",
    "#   const IntegerType zero = Dup<IntegerType>(0);\n",
    "#   const IntegerType one = Dup<IntegerType>(1);\n",
    "#   const IntegerType remainder = BitAnd(x, mask);\n",
    "#   const IntegerType threshold =\n",
    "#       Add(ShiftRight(mask, 1), BitAnd(MaskIfLessThan(x, zero), one));\n",
    "#   return Add(ShiftRight(x, exponent),\n",
    "#              BitAnd(MaskIfGreaterThan(remainder, threshold), one));\n",
    "\n",
    "\n",
    "# I think this is the line where they call the floating-point-multiplier computation:\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/fully_connected.cc#L418\n",
    "# This is the function that is actually called\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/kernel_util.cc#L329\n",
    "# It does the same thing we are doing, with the small caveat that the product is computed in double precision\n",
    "#\n",
    "# Right after that, they call the quantisation function for that multiplier in this line:\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/fully_connected.cc#L421\n",
    "# I think this is the function that's called, although there are five defined functions with that same name\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.cc#L53\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_INT_64 = np.int64(S_UINT)\n",
    "\n",
    "def gemmlowp_requantize(x):\n",
    "    # TODO overflows are disregarded\n",
    "\n",
    "    # TODO is the cast necessary or induced from the type of S_UINT_64\n",
    "    x_s_int_64 = np.int64(x) * S_INT_64\n",
    "    nudge = (1 << 30) if x_s_int_64 >= 0 else (1 - (1 << 30))\n",
    "    nudged = ((x_s_int_64 + nudge) >> 31).astype(np.int32)\n",
    "\n",
    "    # funny (worrying?): nudge can be more than 1 away from the actual float-computed product\n",
    "\n",
    "    mask = (2 ** S_REL_SHIFT) - 1\n",
    "    remainder = nudged & mask\n",
    "    threshold = (mask >> 1) + (1 if nudged < 0 else 0)\n",
    "\n",
    "    return (nudged >> S_REL_SHIFT) + (1 if remainder > threshold else 0)\n",
    "\n",
    "def arm_requantize(x):\n",
    "    # TODO overflows are disregarded\n",
    "\n",
    "    # TODO is the cast necessary or induced from the type of S_UINT_64\n",
    "    x_s_int_64 = np.int64(x) * S_INT_64\n",
    "    nudge = (1 << 30) if x_s_int_64 >= 0 else (1 - (1 << 30))\n",
    "    nudged = ((x_s_int_64 + nudge) >> 31).astype(np.int32)\n",
    "\n",
    "    # funny (worrying?): nudge can be more than 1 away from the actual float-computed product\n",
    "\n",
    "    # TODO handle S_EXPONENT == 0\n",
    "    return (nudged + (1 << (-S_REL_SHIFT - 1))) >> -S_REL_SHIFT\n",
    "\n",
    "requantise_tensor = np.vectorize(arm_requantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_model_accurate(x):\n",
    "    x = (x.reshape(RESHAPE).astype(np.int32) - 128).astype(np.int8)\n",
    "    x = x.astype(np.int32) - Z_I\n",
    "    x = np.matmul(x, W_32) + B_32\n",
    "\n",
    "    # this is the correct, specification-exact way to do it; in the 10000 sample images, it always coincides with np.rint(x * S)\n",
    "    x = requantise_tensor(x)\n",
    "    \n",
    "    x = np.clip(x + Z_O, -128, 127)\n",
    "    x = (x + 128).astype(np.uint8)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for x in quantised_test_x:\n",
    "    interpreter.set_tensor(input_details[\"index\"], x)\n",
    "    interpreter.invoke()\n",
    "    interpreter.get_tensor(output_details[\"index\"])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for x in quantised_test_x:\n",
    "    manual_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for x in quantised_test_x:\n",
    "    manual_model_accurate(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple results\n",
    "\n",
    "- The cumulative execution time of the TF Lite model on the 10000 test images is ~18 ms (average over several runs)\n",
    "- The cumulative execution time of the manual model with naive re-quantisation on the 10000 test images is ~116 ms (idem)\n",
    "- The cumulative execution time of the manual model with specification-exact re-quantisation on the 10000 test images is ~240 ms (idem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrepancies = []\n",
    "\n",
    "for (i, x) in enumerate(quantised_test_x):\n",
    "\n",
    "    # TF Lite model\n",
    "    interpreter.set_tensor(input_details[\"index\"], x)\n",
    "    interpreter.invoke()\n",
    "    tflite_output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    # Manual model\n",
    "    manual_output = manual_model_accurate(x)\n",
    "\n",
    "    if not (tflite_output == manual_output).all():\n",
    "        discrepancies.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(discrepancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = quantised_test_x[discrepancies[0]] \n",
    "\n",
    "interpreter.set_tensor(input_details[\"index\"], ip)\n",
    "interpreter.invoke()\n",
    "out_l = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "out_m = manual_model_accurate(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(v1, v2):\n",
    "    print(v1)\n",
    "    print(v2)\n",
    "    print((v1 == v2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(out_l, out_m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = (ip.reshape(RESHAPE).astype(np.int32) - 128).astype(np.int8)\n",
    "x2 = x1.astype(np.int32) - Z_I\n",
    "x3 = np.matmul(x2, W_32) + B_32\n",
    "x4 = requantise_tensor(x3).astype(np.int32)\n",
    "x5 = np.clip(x4 + Z_O, -128, 127)\n",
    "x6 = (x5 + 128).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC input\n",
    "(x1 == interpreter.get_tensor(5)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC output\n",
    "compare(x5, interpreter.get_tensor(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back of the envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
