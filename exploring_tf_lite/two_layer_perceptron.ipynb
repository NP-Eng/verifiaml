{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from math import log2, ceil, floor\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(HOME_DIR, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_nearest(x, scale, zero, qtype):\n",
    "    if qtype not in {np.int8, np.uint8}:\n",
    "        raise Exception(\"Only quantization to int8 or uint8 is supported\")\n",
    "    \n",
    "    (min, max) = (-128, 127) if qtype == np.int8 else (0, 255)\n",
    "\n",
    "    return np.clip(np.rint(x / scale) + zero, min, max).astype(qtype)\n",
    "\n",
    "def fc_and_requantize(input_tensor, weights, bias, q_i, q_w, q_o):\n",
    "    \n",
    "    if input_tensor.dtype != np.int8:\n",
    "        raise Exception(\"Input must be of type int8\")\n",
    "    \n",
    "    if weights.dtype != np.int8:\n",
    "        raise Exception(\"Weights must be of type int8\")\n",
    "    \n",
    "    if bias.dtype != np.int32:\n",
    "        raise Exception(\"Input and weights must be of type int32\")\n",
    "    \n",
    "    (s_i, z_i), (s_w, z_w), (s_o, z_o) = q_i, q_w, q_o\n",
    "    \n",
    "    if z_w != 0:\n",
    "        raise Exception(\"Expected zero point of weights to be 0\")\n",
    "\n",
    "    s = s_i * s_w / s_o\n",
    "\n",
    "    # 1) shift input tensor\n",
    "    input_tensor_32 = input_tensor.astype(np.int32) - z_i\n",
    "    weights_32 = weights.astype(np.int32)\n",
    "\n",
    "    # 2) compute the bmm\n",
    "    bmm = np.matmul(input_tensor_32, weights_32.transpose()) + bias\n",
    "\n",
    "    # 3) requantize\n",
    "    rq = np.rint(s * bmm) + z_o\n",
    "\n",
    "    # 4) saturating cast\n",
    "    output = np.clip(rq, -228, 127).astype(np.int8)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train.astype(np.float32) / 255.0, x_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(os.path.join(MODEL_DIR, \"two_layer_perceptron_frozen.tflite\"), experimental_preserve_all_tensors=True)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'serving_default_flatten_4_input:0', 'index': 0, 'shape': array([ 1, 28, 28], dtype=int32), 'shape_signature': array([-1, 28, 28], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921568859368563, 0), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "<class 'numpy.uint8'>\n",
      "{'name': 'StatefulPartitionedCall:0', 'index': 10, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.20425860583782196, 159), 'quantization_parameters': {'scales': array([0.2042586], dtype=float32), 'zero_points': array([159], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "<class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "print(input_details)\n",
    "print(input_details[\"dtype\"])\n",
    "\n",
    "print(output_details)\n",
    "print(output_details[\"dtype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_flatten_4_input:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 28, 28], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28], dtype=int32),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.003921568859368563, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_4/flatten_4/Const',\n",
       "  'index': 1,\n",
       "  'shape': array([2], dtype=int32),\n",
       "  'shape_signature': array([2], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_4/dense_7/BiasAdd/ReadVariableOp',\n",
       "  'index': 2,\n",
       "  'shape': array([10], dtype=int32),\n",
       "  'shape_signature': array([10], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0006746734725311399, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00067467], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_4/dense_7/MatMul',\n",
       "  'index': 3,\n",
       "  'shape': array([10, 28], dtype=int32),\n",
       "  'shape_signature': array([10, 28], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.011379142291843891, 0),\n",
       "  'quantization_parameters': {'scales': array([0.01137914], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_4/dense_6/BiasAdd/ReadVariableOp',\n",
       "  'index': 4,\n",
       "  'shape': array([28], dtype=int32),\n",
       "  'shape_signature': array([28], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (2.5658400772954337e-05, 0),\n",
       "  'quantization_parameters': {'scales': array([2.56584e-05], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_4/dense_6/MatMul',\n",
       "  'index': 5,\n",
       "  'shape': array([ 28, 784], dtype=int32),\n",
       "  'shape_signature': array([ 28, 784], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.006542891729623079, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00654289], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'tfl.quantize',\n",
       "  'index': 6,\n",
       "  'shape': array([ 1, 28, 28], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.003921568859368563, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_4/flatten_4/Reshape',\n",
       "  'index': 7,\n",
       "  'shape': array([  1, 784], dtype=int32),\n",
       "  'shape_signature': array([ -1, 784], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.003921568859368563, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_4/dense_6/MatMul;sequential_4/re_lu_2/Relu;sequential_4/dense_6/BiasAdd',\n",
       "  'index': 8,\n",
       "  'shape': array([ 1, 28], dtype=int32),\n",
       "  'shape_signature': array([-1, 28], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.059290364384651184, -128),\n",
       "  'quantization_parameters': {'scales': array([0.05929036], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'StatefulPartitionedCall:01',\n",
       "  'index': 9,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.20425860583782196, 31),\n",
       "  'quantization_parameters': {'scales': array([0.2042586], dtype=float32),\n",
       "   'zero_points': array([31], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 10,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.20425860583782196, 159),\n",
       "  'quantization_parameters': {'scales': array([0.2042586], dtype=float32),\n",
       "   'zero_points': array([159], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_tensor_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: serving_default_flatten_4_input:0\n",
      "1: sequential_4/flatten_4/Const\n",
      "2: sequential_4/dense_7/BiasAdd/ReadVariableOp\n",
      "3: sequential_4/dense_7/MatMul\n",
      "4: sequential_4/dense_6/BiasAdd/ReadVariableOp\n",
      "5: sequential_4/dense_6/MatMul\n",
      "6: tfl.quantize\n",
      "7: sequential_4/flatten_4/Reshape\n",
      "8: sequential_4/dense_6/MatMul;sequential_4/re_lu_2/Relu;sequential_4/dense_6/BiasAdd\n",
      "9: StatefulPartitionedCall:01\n",
      "10: StatefulPartitionedCall:0\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(interpreter.get_tensor_details()):\n",
    "    print(i, \": \", t[\"name\"], sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0,\n",
       "  'op_name': 'QUANTIZE',\n",
       "  'inputs': array([0], dtype=int32),\n",
       "  'outputs': array([6], dtype=int32)},\n",
       " {'index': 1,\n",
       "  'op_name': 'RESHAPE',\n",
       "  'inputs': array([6, 1], dtype=int32),\n",
       "  'outputs': array([7], dtype=int32)},\n",
       " {'index': 2,\n",
       "  'op_name': 'FULLY_CONNECTED',\n",
       "  'inputs': array([7, 5, 4], dtype=int32),\n",
       "  'outputs': array([8], dtype=int32)},\n",
       " {'index': 3,\n",
       "  'op_name': 'FULLY_CONNECTED',\n",
       "  'inputs': array([8, 3, 2], dtype=int32),\n",
       "  'outputs': array([9], dtype=int32)},\n",
       " {'index': 4,\n",
       "  'op_name': 'QUANTIZE',\n",
       "  'inputs': array([9], dtype=int32),\n",
       "  'outputs': array([10], dtype=int32)}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter._get_ops_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1_idx = 7\n",
    "weight_1_idx = 5\n",
    "bias_1_idx = 4\n",
    "output_1_idx = 8\n",
    "\n",
    "w_1 = interpreter.get_tensor(weight_1_idx)\n",
    "b_1 = interpreter.get_tensor(bias_1_idx)\n",
    "q_1_i = interpreter.get_tensor_details()[input_1_idx][\"quantization\"]\n",
    "q_1_w = interpreter.get_tensor_details()[weight_1_idx][\"quantization\"]\n",
    "q_1_o = interpreter.get_tensor_details()[output_1_idx][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2_idx = 8\n",
    "weight_2_idx = 3\n",
    "bias_2_idx = 2\n",
    "output_2_idx = 9\n",
    "\n",
    "w_2 = interpreter.get_tensor(weight_2_idx)\n",
    "b_2 = interpreter.get_tensor(bias_2_idx)\n",
    "q_2_i = interpreter.get_tensor_details()[input_2_idx][\"quantization\"]\n",
    "q_2_w = interpreter.get_tensor_details()[weight_2_idx][\"quantization\"]\n",
    "q_2_o = interpreter.get_tensor_details()[output_2_idx][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "peek = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_model(input_tensor):\n",
    "\n",
    "    global peek\n",
    "\n",
    "    # 1) shift input tensor by -128 to sitch from input type (uint8) to TF Lite internal type (int8) \n",
    "    shifted_input = input_tensor.astype(np.int32)\n",
    "    shifted_input = shifted_input - 128\n",
    "    shifted_input = shifted_input.astype(np.int8)\n",
    "\n",
    "    # 2) flatten input\n",
    "    flattened_input = shifted_input.reshape(interpreter.get_tensor(1)) # [-1, 784]\n",
    "    \n",
    "    # 3) first fully-connected layer\n",
    "    fc1 = fc_and_requantize(flattened_input, w_1, b_1, q_1_i, q_1_w, q_1_o)\n",
    "\n",
    "    # 4) relu\n",
    "    print(fc1)\n",
    "    relu1 = np.maximum(fc1, 0)\n",
    "    print(relu1)\n",
    "\n",
    "    peek = fc1\n",
    "\n",
    "    # 5) second fully-connected layer\n",
    "    fc2 = fc_and_requantize(relu1, w_2, b_2, q_2_i, q_2_w, q_2_o)\n",
    "\n",
    "    # 4) undo the shift to switch from TF Lite internal type (int8) to output type (uint8)\n",
    "    output = fc2.astype(np.int32)\n",
    "    output = output + 128\n",
    "    output = output.astype(np.uint8)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_image = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -23  -67  -43 -102 -109 -120  112 -108   71  -79  111 -120  -56  -82\n",
      "  -116   -8  -78  -46  -85 -109  -88 -114  123  116   80  -69  -58 -110]]\n",
      "[[  0   0   0   0   0   0 112   0  71   0 111   0   0   0   0   0   0   0\n",
      "    0   0   0   0 123 116  80   0   0   0]]\n",
      "Test image 150\n",
      "TF Lite output:\t\t[138 106 149 160 174 152 141 146 169 207]\n",
      "Manual model output:\t[160 132 139 162  68 145  85 200  16  28]\n",
      "Correct label: 9\n"
     ]
    }
   ],
   "source": [
    "test_image = x_test[chosen_image]\n",
    "\n",
    "# Need to quantize the inputs outside the model!\n",
    "input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "input_tensor = quantize_nearest(test_image, input_scale, input_zero_point, np.uint8)\n",
    "input_tensor = np.expand_dims(input_tensor, axis=0)\n",
    "\n",
    "# Run the TF Lite model\n",
    "interpreter.set_tensor(input_details[\"index\"], input_tensor)\n",
    "interpreter.invoke()\n",
    "tflite_output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "# Run the manual model\n",
    "manual_output = manual_model(input_tensor)[0]\n",
    "\n",
    "print(\"Test image {}\\nTF Lite output:\\t\\t{}\\nManual model output:\\t{}\\nCorrect label: {}\".format(chosen_image, tflite_output, manual_output, y_test[chosen_image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(manual_output == tflite_output).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=30, suppress=True)\n",
    "# x_test[chosen_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_2d(t):\n",
    "    for st in t[0]:\n",
    "        print(\"[{}],\".format(\", \".join([str(e) for e in st])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 254, 254, 215, 163, 163, 143, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 70, 192, 198, 198, 198, 234, 253, 237, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 86, 253, 253, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 80, 253, 253, 137, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 228, 253, 227, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 228, 255, 238, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 166, 253, 238, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 73, 222, 253, 207, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 138, 253, 253, 168, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 162, 253, 253, 156, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 111, 207, 255, 234, 75, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 134, 249, 237, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 113, 253, 244, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 61, 253, 253, 143, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 27, 183, 253, 253, 98, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 99, 254, 254, 242, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72, 247, 253, 245, 87, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 86, 36, 82, 243, 254, 245, 121, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 90, 249, 233, 253, 253, 209, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 240, 253, 246, 129, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"
     ]
    }
   ],
   "source": [
    "print_2d(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying parameters for Rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.003921568859368563, 0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_input_details()[0][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.003921568859368563, -128)\n",
      "(0.006542891729623079, 0)\n",
      "(0.059290364384651184, -128)\n",
      "(0.059290364384651184, -128)\n",
      "(0.011379142291843891, 0)\n",
      "(0.20425860583782196, 31)\n"
     ]
    }
   ],
   "source": [
    "print(q_1_i, q_1_w, q_1_o, sep=\"\\n\")\n",
    "print(q_2_i, q_2_w, q_2_o, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -17,\n",
      "    23,\n",
      "    23,\n",
      "    -24,\n",
      "    2,\n",
      "    4,\n",
      "    19,\n",
      "    26,\n",
      "    2,\n",
      "    -34,\n",
      "    36,\n",
      "    32,\n",
      "    -79,\n",
      "    41,\n",
      "    -41,\n",
      "    25,\n",
      "    40,\n",
      "    -27,\n",
      "    21,\n",
      "    1,\n",
      "    15,\n",
      "    -57,\n",
      "    56,\n",
      "    -17,\n",
      "    -40,\n",
      "    -49,\n",
      "    -32,\n",
      "    -7,\n",
      "    -10,\n",
      "    -31,\n",
      "    -87,\n",
      "    24,\n",
      "    20,\n",
      "    -35,\n",
      "    -17,\n",
      "    -84,\n",
      "    39,\n",
      "    -28,\n",
      "    -16,\n",
      "    -43,\n",
      "    41,\n",
      "    -68,\n",
      "    25,\n",
      "    -56,\n",
      "    42,\n",
      "    39,\n",
      "    -17,\n",
      "    -10,\n",
      "    74,\n",
      "    29,\n",
      "    30,\n",
      "    37,\n",
      "    22,\n",
      "    -56,\n",
      "    -24,\n",
      "    24,\n",
      "    40,\n",
      "    -10,\n",
      "    0,\n",
      "    -109,\n",
      "    30,\n",
      "    2,\n",
      "    33,\n",
      "    45,\n",
      "    -102,\n",
      "    3,\n",
      "    -15,\n",
      "    7,\n",
      "    -33,\n",
      "    48,\n",
      "    34,\n",
      "    -39,\n",
      "    28,\n",
      "    -20,\n",
      "    -20,\n",
      "    -63,\n",
      "    -22,\n",
      "    11,\n",
      "    8,\n",
      "    52,\n",
      "    -3,\n",
      "    31,\n",
      "    -3,\n",
      "    4,\n",
      "    32,\n",
      "    -18,\n",
      "    -60,\n",
      "    50,\n",
      "    -22,\n",
      "    -31,\n",
      "    -22,\n",
      "    -31,\n",
      "    -57,\n",
      "    37,\n",
      "    37,\n",
      "    16,\n",
      "    -24,\n",
      "    16,\n",
      "    -19,\n",
      "    5,\n",
      "    8,\n",
      "    -34,\n",
      "    -9,\n",
      "    37,\n",
      "    1,\n",
      "    22,\n",
      "    -16,\n",
      "    36,\n",
      "    15,\n",
      "    35,\n",
      "    34,\n",
      "    -38,\n",
      "    34,\n",
      "    -31,\n",
      "    50,\n",
      "    24,\n",
      "    -21,\n",
      "    25,\n",
      "    -46,\n",
      "    20,\n",
      "    13,\n",
      "    -28,\n",
      "    -10,\n",
      "    44,\n",
      "    44,\n",
      "    31,\n",
      "    -38,\n",
      "    -7,\n",
      "    29,\n",
      "    11,\n",
      "    -17,\n",
      "    -3,\n",
      "    -84,\n",
      "    43,\n",
      "    -56,\n",
      "    -33,\n",
      "    -29,\n",
      "    10,\n",
      "    -60,\n",
      "    13,\n",
      "    20,\n",
      "    35,\n",
      "    21,\n",
      "    -26,\n",
      "    -102,\n",
      "    58,\n",
      "    27,\n",
      "    -1,\n",
      "    45,\n",
      "    9,\n",
      "    -24,\n",
      "    35,\n",
      "    -15,\n",
      "    -28,\n",
      "    -34,\n",
      "    -13,\n",
      "    -43,\n",
      "    2,\n",
      "    19,\n",
      "    16,\n",
      "    59,\n",
      "    22,\n",
      "    -41,\n",
      "    -20,\n",
      "    35,\n",
      "    -61,\n",
      "    -11,\n",
      "    -13,\n",
      "    -62,\n",
      "    28,\n",
      "    30,\n",
      "    -64,\n",
      "    1,\n",
      "    -71,\n",
      "    -3,\n",
      "    20,\n",
      "    27,\n",
      "    29,\n",
      "    -29,\n",
      "    -32,\n",
      "    40,\n",
      "    29,\n",
      "    -44,\n",
      "    -57,\n",
      "    35,\n",
      "    34,\n",
      "    -10,\n",
      "    14,\n",
      "    38,\n",
      "    -7,\n",
      "    14,\n",
      "    11,\n",
      "    -66,\n",
      "    40,\n",
      "    -112,\n",
      "    26,\n",
      "    29,\n",
      "    -32,\n",
      "    22,\n",
      "    10,\n",
      "    44,\n",
      "    38,\n",
      "    11,\n",
      "    -8,\n",
      "    10,\n",
      "    -60,\n",
      "    23,\n",
      "    29,\n",
      "    -33,\n",
      "    -9,\n",
      "    4,\n",
      "    -18,\n",
      "    -39,\n",
      "    47,\n",
      "    -67,\n",
      "    40,\n",
      "    -28,\n",
      "    1,\n",
      "    35,\n",
      "    73,\n",
      "    -80,\n",
      "    13,\n",
      "    15,\n",
      "    -57,\n",
      "    12,\n",
      "    -16,\n",
      "    41,\n",
      "    -66,\n",
      "    -48,\n",
      "    -55,\n",
      "    7,\n",
      "    -47,\n",
      "    -14,\n",
      "    24,\n",
      "    -57,\n",
      "    -13,\n",
      "    -6,\n",
      "    20,\n",
      "    29,\n",
      "    33,\n",
      "    2,\n",
      "    -58,\n",
      "    30,\n",
      "    33,\n",
      "    -74,\n",
      "    31,\n",
      "    -50,\n",
      "    -35,\n",
      "    -2,\n",
      "    26,\n",
      "    10,\n",
      "    39,\n",
      "    35,\n",
      "    7,\n",
      "    6,\n",
      "    51,\n",
      "    17,\n",
      "    23,\n",
      "    -72,\n",
      "    -21,\n",
      "    -10,\n",
      "    -24,\n",
      "    -51,\n",
      "    -16,\n",
      "    8,\n",
      "    18,\n",
      "    -21,\n",
      "    38,\n",
      "    35,\n",
      "    26,\n",
      "    47,\n",
      "    1,\n",
      "    -34,\n",
      "    -56,\n",
      "    0,\n",
      "    -127,\n",
      "    29,\n",
      "    24,\n",
      "    -8,\n",
      "    -24,\n"
     ]
    }
   ],
   "source": [
    "for e in w_2.flatten():\n",
    "    print(\"    \", e, \",\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -17,\n",
      "    23,\n",
      "    23,\n",
      "    -24,\n",
      "    2,\n",
      "    4,\n",
      "    19,\n",
      "    26,\n",
      "    2,\n",
      "    -34,\n",
      "    36,\n",
      "    32,\n",
      "    -79,\n",
      "    41,\n",
      "    -41,\n",
      "    25,\n",
      "    40,\n",
      "    -27,\n",
      "    21,\n",
      "    1,\n",
      "    15,\n",
      "    -57,\n",
      "    56,\n",
      "    -17,\n",
      "    -40,\n",
      "    -49,\n",
      "    -32,\n",
      "    -7,\n",
      "    -10,\n",
      "    -31,\n",
      "    -87,\n",
      "    24,\n",
      "    20,\n",
      "    -35,\n",
      "    -17,\n",
      "    -84,\n",
      "    39,\n",
      "    -28,\n",
      "    -16,\n",
      "    -43,\n",
      "    41,\n",
      "    -68,\n",
      "    25,\n",
      "    -56,\n",
      "    42,\n",
      "    39,\n",
      "    -17,\n",
      "    -10,\n",
      "    74,\n",
      "    29,\n",
      "    30,\n",
      "    37,\n",
      "    22,\n",
      "    -56,\n",
      "    -24,\n",
      "    24,\n",
      "    40,\n",
      "    -10,\n",
      "    0,\n",
      "    -109,\n",
      "    30,\n",
      "    2,\n",
      "    33,\n",
      "    45,\n",
      "    -102,\n",
      "    3,\n",
      "    -15,\n",
      "    7,\n",
      "    -33,\n",
      "    48,\n",
      "    34,\n",
      "    -39,\n",
      "    28,\n",
      "    -20,\n",
      "    -20,\n",
      "    -63,\n",
      "    -22,\n",
      "    11,\n",
      "    8,\n",
      "    52,\n",
      "    -3,\n",
      "    31,\n",
      "    -3,\n",
      "    4,\n",
      "    32,\n",
      "    -18,\n",
      "    -60,\n",
      "    50,\n",
      "    -22,\n",
      "    -31,\n",
      "    -22,\n",
      "    -31,\n",
      "    -57,\n",
      "    37,\n",
      "    37,\n",
      "    16,\n",
      "    -24,\n",
      "    16,\n",
      "    -19,\n",
      "    5,\n",
      "    8,\n",
      "    -34,\n",
      "    -9,\n",
      "    37,\n",
      "    1,\n",
      "    22,\n",
      "    -16,\n",
      "    36,\n",
      "    15,\n",
      "    35,\n",
      "    34,\n",
      "    -38,\n",
      "    34,\n",
      "    -31,\n",
      "    50,\n",
      "    24,\n",
      "    -21,\n",
      "    25,\n",
      "    -46,\n",
      "    20,\n",
      "    13,\n",
      "    -28,\n",
      "    -10,\n",
      "    44,\n",
      "    44,\n",
      "    31,\n",
      "    -38,\n",
      "    -7,\n",
      "    29,\n",
      "    11,\n",
      "    -17,\n",
      "    -3,\n",
      "    -84,\n",
      "    43,\n",
      "    -56,\n",
      "    -33,\n",
      "    -29,\n",
      "    10,\n",
      "    -60,\n",
      "    13,\n",
      "    20,\n",
      "    35,\n",
      "    21,\n",
      "    -26,\n",
      "    -102,\n",
      "    58,\n",
      "    27,\n",
      "    -1,\n",
      "    45,\n",
      "    9,\n",
      "    -24,\n",
      "    35,\n",
      "    -15,\n",
      "    -28,\n",
      "    -34,\n",
      "    -13,\n",
      "    -43,\n",
      "    2,\n",
      "    19,\n",
      "    16,\n",
      "    59,\n",
      "    22,\n",
      "    -41,\n",
      "    -20,\n",
      "    35,\n",
      "    -61,\n",
      "    -11,\n",
      "    -13,\n",
      "    -62,\n",
      "    28,\n",
      "    30,\n",
      "    -64,\n",
      "    1,\n",
      "    -71,\n",
      "    -3,\n",
      "    20,\n",
      "    27,\n",
      "    29,\n",
      "    -29,\n",
      "    -32,\n",
      "    40,\n",
      "    29,\n",
      "    -44,\n",
      "    -57,\n",
      "    35,\n",
      "    34,\n",
      "    -10,\n",
      "    14,\n",
      "    38,\n",
      "    -7,\n",
      "    14,\n",
      "    11,\n",
      "    -66,\n",
      "    40,\n",
      "    -112,\n",
      "    26,\n",
      "    29,\n",
      "    -32,\n",
      "    22,\n",
      "    10,\n",
      "    44,\n",
      "    38,\n",
      "    11,\n",
      "    -8,\n",
      "    10,\n",
      "    -60,\n",
      "    23,\n",
      "    29,\n",
      "    -33,\n",
      "    -9,\n",
      "    4,\n",
      "    -18,\n",
      "    -39,\n",
      "    47,\n",
      "    -67,\n",
      "    40,\n",
      "    -28,\n",
      "    1,\n",
      "    35,\n",
      "    73,\n",
      "    -80,\n",
      "    13,\n",
      "    15,\n",
      "    -57,\n",
      "    12,\n",
      "    -16,\n",
      "    41,\n",
      "    -66,\n",
      "    -48,\n",
      "    -55,\n",
      "    7,\n",
      "    -47,\n",
      "    -14,\n",
      "    24,\n",
      "    -57,\n",
      "    -13,\n",
      "    -6,\n",
      "    20,\n",
      "    29,\n",
      "    33,\n",
      "    2,\n",
      "    -58,\n",
      "    30,\n",
      "    33,\n",
      "    -74,\n",
      "    31,\n",
      "    -50,\n",
      "    -35,\n",
      "    -2,\n",
      "    26,\n",
      "    10,\n",
      "    39,\n",
      "    35,\n",
      "    7,\n",
      "    6,\n",
      "    51,\n",
      "    17,\n",
      "    23,\n",
      "    -72,\n",
      "    -21,\n",
      "    -10,\n",
      "    -24,\n",
      "    -51,\n",
      "    -16,\n",
      "    8,\n",
      "    18,\n",
      "    -21,\n",
      "    38,\n",
      "    35,\n",
      "    26,\n",
      "    47,\n",
      "    1,\n",
      "    -34,\n",
      "    -56,\n",
      "    0,\n",
      "    -127,\n",
      "    29,\n",
      "    24,\n",
      "    -8,\n",
      "    -24,\n"
     ]
    }
   ],
   "source": [
    "for e in w_2.flatten():\n",
    "    print(\"    \", e, \",\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -300,\n",
      "    288,\n",
      "    384,\n",
      "    -241,\n",
      "    73,\n",
      "    423,\n",
      "    -301,\n",
      "    -47,\n",
      "    -173,\n",
      "    -227,\n"
     ]
    }
   ],
   "source": [
    "for e in b_2.flatten():\n",
    "    print(\"    \", e, \",\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back of the envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fc1_tflite = interpreter.get_tensor(7)\n",
    "post_fc1_relu_tflite = interpreter.get_tensor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -87, -100, -128, -106,  -73, -127, -101, -112,  -76,    5,  -92,\n",
       "        -120, -128,  -81,  -81,  -54,  -82, -128,  -80,  -74,  -70,  -91,\n",
       "        -128, -110,  -66, -128,  -52, -105]], dtype=int8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_tensor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -87, -100,  119, -106,  -73, -127, -101, -112,  -76,    5,  -92,\n",
       "        -120,  115,  -81,  -81,  -54,  -82, -128,  -80,  -74,  -70,  -91,\n",
       "         117, -110,  -66,   82,  -52, -105]], dtype=int8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_default_flatten_4_input:0\n",
      "sequential_4/flatten_4/Const\n",
      "sequential_4/dense_7/BiasAdd/ReadVariableOp\n",
      "sequential_4/dense_7/MatMul\n",
      "sequential_4/dense_6/BiasAdd/ReadVariableOp\n",
      "sequential_4/dense_6/MatMul\n",
      "tfl.quantize\n",
      "sequential_4/flatten_4/Reshape\n",
      "sequential_4/dense_6/MatMul;sequential_4/re_lu_2/Relu;sequential_4/dense_6/BiasAdd\n",
      "StatefulPartitionedCall:01\n",
      "StatefulPartitionedCall:0\n"
     ]
    }
   ],
   "source": [
    "for t in interpreter.get_tensor_details():\n",
    "    print(t[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -87, -100, -128, -106,  -73, -127, -101, -112,  -76,    5,  -92,\n",
       "        -120, -128,  -81,  -81,  -54,  -82, -128,  -80,  -74,  -70,  -91,\n",
       "        -128, -110,  -66, -128,  -52, -105]], dtype=int8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_tensor(8)\n",
    "# [[ -87 -100  119 -106  -73 -127 -101 -112  -76    5  -92 -120  115  -81\n",
    "#    -81  -54  -82 -128  -80  -74  -70  -91  117 -110  -66   82  -52 -105]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi [ -87, -100,  119, -106,  -73, -127, -101, -112,  -76,  5,  -92, -120,  115,  -81,  -81,  -54,  -82, -128,  -80,  -74,  -70,  -91,  117, -110,  -66,   82,  -52, -105]\n",
    "# in [ -87, -100, -128, -106,  -73, -127, -101, -112,  -76,  5,  -92, -120, -128,  -81,  -81,  -54,  -82, -128,  -80,  -74,  -70,  -91, -128, -110,  -66, -128,  -52, -105]\n",
    "#                  h                                                                                                                    h                 h                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
